{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90ae34a-8ff5-4e81-9055-9933bdc60699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aga\n",
      "lgd2\n"
     ]
    }
   ],
   "source": [
    "print('aga')\n",
    "!echo $CONDA_DEFAULT_ENV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e36b1ad-297a-4b5d-976a-cedc9366dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lgd  # noqa, register custom modules\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from torch_geometric.graphgym.cmd_args import parse_args\n",
    "from torch_geometric.graphgym.config import (cfg, dump_cfg,\n",
    "                                             set_cfg, load_cfg,\n",
    "                                             makedirs_rm_exist)\n",
    "from torch_geometric.graphgym.loader import create_loader\n",
    "from torch_geometric.graphgym.logger import set_printing\n",
    "from torch_geometric.graphgym.optim import create_optimizer, \\\n",
    "    create_scheduler, OptimizerConfig\n",
    "from torch_geometric.graphgym.model_builder import create_model\n",
    "from torch_geometric.graphgym.train import GraphGymDataModule, train\n",
    "from torch_geometric.graphgym.utils.comp_budget import params_count\n",
    "from torch_geometric.graphgym.utils.device import auto_select_device\n",
    "from torch_geometric.graphgym.register import train_dict, network_dict, register_network, act_dict\n",
    "from torch_geometric import seed_everything\n",
    "from lgd.asset.logger import create_logger\n",
    "from lgd.loader.master_loader import load_dataset_master\n",
    "from lgd.optimizer.extra_optimizers import ExtendedSchedulerConfig\n",
    "from lgd.agg_runs import agg_runs\n",
    "from lgd.finetuning import load_pretrained_model_cfg, \\\n",
    "    init_model_from_pretrained\n",
    "from lgd.ddpm.LGD import DDPM, LatentDiffusion\n",
    "from lgd.ddpm.LGD_Inductive import LatentDiffusionInductive\n",
    "from lgd.train.pretrain_encoder import *\n",
    "from lgd.encoder.atom_bond_encoder import *\n",
    "from lgd.model.GraphTransformerEncoder import GraphTransformerEncoder # HERE \n",
    "from lgd.train.train_diffusion import *\n",
    "from utils import print_gpu_usage\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "from pretrain import *\n",
    "from lgd.model.GraphTransformerEncoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46aebe06-1ac4-4f9b-925d-3aa05ced47f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here ogbg-molhiv\n",
      "if ten once ./datasets/ogbg_molhiv/processed.pt\n",
      "if ici\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eren/LatentGraphDiffusion/lgd/loader/master_loader.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(processed_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerator: cpu\n",
      "benchmark: False\n",
      "bn:\n",
      "  eps: 1e-05\n",
      "  mom: 0.1\n",
      "cfg_dest: config.yaml\n",
      "custom_metrics: []\n",
      "dataset:\n",
      "  add_virtual_node_edge: True\n",
      "  cache_load: False\n",
      "  cache_save: False\n",
      "  dir: ./datasets\n",
      "  edge_dim: 128\n",
      "  edge_encoder: True\n",
      "  edge_encoder_bn: False\n",
      "  edge_encoder_name: Bond_pad\n",
      "  edge_encoder_num_types: 5\n",
      "  edge_message_ratio: 0.8\n",
      "  edge_negative_sampling_ratio: 1.0\n",
      "  edge_train_mode: all\n",
      "  encoder: True\n",
      "  encoder_bn: True\n",
      "  encoder_dim: 128\n",
      "  encoder_name: db\n",
      "  format: OGB\n",
      "  label_column: none\n",
      "  label_table: none\n",
      "  location: local\n",
      "  name: ogbg-molhiv\n",
      "  node_encoder: True\n",
      "  node_encoder_bn: False\n",
      "  node_encoder_name: Atom_pad\n",
      "  node_encoder_num_types: 120\n",
      "  remove_feature: False\n",
      "  resample_disjoint: False\n",
      "  resample_negative: False\n",
      "  shuffle_split: True\n",
      "  split: [0.8, 0.1, 0.1]\n",
      "  split_index: 0\n",
      "  split_mode: standard\n",
      "  subgraph: False\n",
      "  task: graph\n",
      "  task_type: classification\n",
      "  to_undirected: False\n",
      "  transductive: False\n",
      "  transform: none\n",
      "  tu_simple: True\n",
      "devices: None\n",
      "diffusion:\n",
      "  cond_stage_config: __is_first_stage__\n",
      "  cond_stage_key: prompt_graph\n",
      "  conditioning_key: crossattn\n",
      "  edge_factor: 1.0\n",
      "  first_stage_config: results/ogbg-molhiv-encoder-ogbg-molhiv-orig-2025-02-21_11-19-17/0/ckpt/693.ckpt\n",
      "  graph_factor: 1.0\n",
      "  hid_dim: 4\n",
      "  node_factor: 1.0\n",
      "  task_factor: 1.0\n",
      "dt:\n",
      "  O_e: True\n",
      "  act: relu\n",
      "  attn:\n",
      "    O_e: True\n",
      "    act: relu\n",
      "    attn_product: mul\n",
      "    attn_reweight: False\n",
      "    bn_momentum: 0.1\n",
      "    bn_no_runner: False\n",
      "    clamp: 5.0\n",
      "    deg_scaler: False\n",
      "    edge_enhance: False\n",
      "    edge_reweight: False\n",
      "    full_attn: True\n",
      "    fwl: False\n",
      "    norm_e: True\n",
      "    score_act: False\n",
      "    signed_sqrt: True\n",
      "  attn_dropout: 0.25\n",
      "  batch_norm: False\n",
      "  bn_momentum: 0.1\n",
      "  bn_no_runner: False\n",
      "  cond_dim: 4\n",
      "  condition_list: ['prompt_graph']\n",
      "  dropout: 0.05\n",
      "  ff_e: True\n",
      "  ff_e_ca: True\n",
      "  ff_e_sa: False\n",
      "  final_norm: True\n",
      "  hid_dim: 64\n",
      "  in_dim: 4\n",
      "  layer_norm: True\n",
      "  norm_e: True\n",
      "  num_heads: 4\n",
      "  num_layers: 4\n",
      "  out_dim: 4\n",
      "  pool: mean\n",
      "  pool_edge: True\n",
      "  pool_vn: False\n",
      "  post_pool: True\n",
      "  residual: True\n",
      "  self_attn: False\n",
      "  temb_dim: 64\n",
      "  use_time: True\n",
      "encoder:\n",
      "  O_e: True\n",
      "  act: relu\n",
      "  add_virtual_node_edge: True\n",
      "  attn:\n",
      "    O_e: True\n",
      "    act: relu\n",
      "    attn_product: mul\n",
      "    attn_reweight: False\n",
      "    bn_momentum: 0.1\n",
      "    bn_no_runner: False\n",
      "    clamp: 5.0\n",
      "    deg_scaler: False\n",
      "    edge_enhance: True\n",
      "    edge_reweight: False\n",
      "    full_attn: True\n",
      "    fwl: False\n",
      "    norm_e: True\n",
      "    score_act: False\n",
      "    signed_sqrt: True\n",
      "  attn_dropout: 0.5\n",
      "  batch_norm: True\n",
      "  bn_momentum: 0.1\n",
      "  bn_no_runner: False\n",
      "  dropout: 0.05\n",
      "  edge_encoder: True\n",
      "  edge_encoder_bn: False\n",
      "  edge_encoder_name: Bond_pad\n",
      "  edge_encoder_num_types: 5\n",
      "  ff_e: False\n",
      "  final_norm: True\n",
      "  force_undirected: True\n",
      "  hid_dim: 64\n",
      "  in_dim: 40\n",
      "  label_embed_type: add_virtual\n",
      "  layer_norm: False\n",
      "  model_type: GraphTransformerStructureEncoder\n",
      "  mpnn:\n",
      "    act: relu\n",
      "    dropout: 0.05\n",
      "    edge_enhance: True\n",
      "    enable: True\n",
      "    project_edge: True\n",
      "  node_encoder: True\n",
      "  node_encoder_bn: False\n",
      "  node_encoder_name: Atom_pad\n",
      "  node_encoder_num_types: 120\n",
      "  norm_e: True\n",
      "  num_classes: 2\n",
      "  num_heads: 4\n",
      "  num_layers: 10\n",
      "  num_task: 1\n",
      "  out_dim: 4\n",
      "  pe_raw_norm: None\n",
      "  pool: mean\n",
      "  pool_edge: True\n",
      "  pool_vn: False\n",
      "  posenc_dim: 24\n",
      "  posenc_in_dim: 16\n",
      "  posenc_in_dim_edge: 16\n",
      "  post_pool: True\n",
      "  prefix_dim: 64\n",
      "  prefix_type: add_virtual\n",
      "  residual: True\n",
      "  task_type: classification\n",
      "  temb_dim: 0\n",
      "  update_e: True\n",
      "  use_time: False\n",
      "gnn:\n",
      "  act: relu\n",
      "  agg: mean\n",
      "  att_final_linear: False\n",
      "  att_final_linear_bn: False\n",
      "  att_heads: 1\n",
      "  batchnorm: True\n",
      "  clear_feature: True\n",
      "  dim_inner: 64\n",
      "  dropout: 0.0\n",
      "  head: san_graph\n",
      "  keep_edge: 0.5\n",
      "  l2norm: True\n",
      "  layer_type: generalconv\n",
      "  layers_mp: 2\n",
      "  layers_post_mp: 3\n",
      "  layers_pre_mp: 0\n",
      "  msg_direction: single\n",
      "  normalize_adj: False\n",
      "  self_msg: concat\n",
      "  skip_every: 1\n",
      "  stage_type: stack\n",
      "gpu_mem: False\n",
      "gt:\n",
      "  attn_dropout: 0.5\n",
      "  batch_norm: True\n",
      "  dim_hidden: 64\n",
      "  dropout: 0.05\n",
      "  layer_norm: False\n",
      "  layer_type: CustomGatedGCN+Transformer\n",
      "  layers: 10\n",
      "  n_heads: 4\n",
      "mem:\n",
      "  inplace: False\n",
      "metric_agg: argmax\n",
      "metric_best: auc\n",
      "model:\n",
      "  edge_decoding: dot\n",
      "  graph_pooling: mean\n",
      "  loss_fun: cross_entropy\n",
      "  match_upper: True\n",
      "  size_average: mean\n",
      "  thresh: 0.5\n",
      "  type: LatentDiffusion\n",
      "name_tag: ogbg-molhiv-orig\n",
      "num_threads: 6\n",
      "num_workers: 0\n",
      "optim:\n",
      "  base_lr: 0.0001\n",
      "  batch_accumulation: 1\n",
      "  clip_grad_norm: True\n",
      "  lr_decay: 0.1\n",
      "  max_epoch: 500\n",
      "  min_lr: 1e-06\n",
      "  momentum: 0.9\n",
      "  num_warmup_epochs: 20\n",
      "  optimizer: adamW\n",
      "  reduce_factor: 0.5\n",
      "  schedule_patience: 5\n",
      "  scheduler: cosine_with_warmup\n",
      "  steps: [30, 60, 90]\n",
      "  weight_decay: 1e-05\n",
      "out_dir: results\n",
      "posenc_ERE:\n",
      "  accuracy: 0.1\n",
      "  dim_pe: 16\n",
      "  enable: None\n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_ERN:\n",
      "  accuracy: 0.1\n",
      "  dim_pe: 16\n",
      "  enable: None\n",
      "  er_dim: none\n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_EdgeRWSE:\n",
      "  dim_pe: 16\n",
      "  enable: False\n",
      "  kernel:\n",
      "    times: []\n",
      "    times_func: \n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_ElstaticSE:\n",
      "  dim_pe: 16\n",
      "  enable: False\n",
      "  kernel:\n",
      "    times: []\n",
      "    times_func: range(10)\n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_EquivStableLapPE:\n",
      "  eigen:\n",
      "    eigvec_norm: L2\n",
      "    laplacian_norm: sym\n",
      "    max_freqs: 10\n",
      "  enable: False\n",
      "  raw_norm_type: none\n",
      "posenc_HKdiagSE:\n",
      "  dim_pe: 16\n",
      "  enable: False\n",
      "  kernel:\n",
      "    times: []\n",
      "    times_func: \n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_HodgeLap1PE:\n",
      "  dim_pe: 16\n",
      "  eigen:\n",
      "    eigvec_norm: L2\n",
      "    laplacian_norm: sym\n",
      "    max_freqs: 10\n",
      "  enable: False\n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_InterRWSE:\n",
      "  dim_pe: 16\n",
      "  enable: False\n",
      "  kernel:\n",
      "    times: []\n",
      "    times_func: \n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_LapPE:\n",
      "  dim_pe: 16\n",
      "  eigen:\n",
      "    eigvec_norm: L2\n",
      "    laplacian_norm: sym\n",
      "    max_freqs: 10\n",
      "  enable: False\n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_RD:\n",
      "  dim_pe: 16\n",
      "  enable: False\n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_RRWP:\n",
      "  add_identity: True\n",
      "  dim_pe: 16\n",
      "  enable: True\n",
      "  ksteps: 16\n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "  real_emb: True\n",
      "  spd: False\n",
      "posenc_RWSE:\n",
      "  dim_pe: 16\n",
      "  enable: False\n",
      "  kernel:\n",
      "    times: []\n",
      "    times_func: \n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "posenc_SignNet:\n",
      "  dim_pe: 16\n",
      "  eigen:\n",
      "    eigvec_norm: L2\n",
      "    laplacian_norm: sym\n",
      "    max_freqs: 10\n",
      "  enable: False\n",
      "  layers: 3\n",
      "  local: False\n",
      "  model: none\n",
      "  n_heads: 4\n",
      "  pass_as_var: False\n",
      "  phi_hidden_dim: 64\n",
      "  phi_out_dim: 4\n",
      "  post_layers: 0\n",
      "  raw_norm_type: none\n",
      "prep:\n",
      "  dist_enable: None\n",
      "  exp: None\n",
      "pretrained:\n",
      "  dir: None\n",
      "print: both\n",
      "round: 4\n",
      "run_dir: results\n",
      "run_multiple_splits: []\n",
      "seed: 0\n",
      "share:\n",
      "  dim_in: 9\n",
      "  dim_out: 2\n",
      "  num_splits: 3\n",
      "tensorboard_agg: True\n",
      "tensorboard_each_run: False\n",
      "train:\n",
      "  auto_resume: False\n",
      "  batch_size: 64\n",
      "  ckpt_best: True\n",
      "  ckpt_clean: False\n",
      "  ckpt_period: 50\n",
      "  enable_ckpt: True\n",
      "  ensemble_mode: none\n",
      "  ensemble_repeat: 1\n",
      "  epoch_resume: -1\n",
      "  eval_period: 50\n",
      "  iter_per_epoch: 32\n",
      "  mode: train_diffusion\n",
      "  neighbor_sizes: [20, 15, 10, 5]\n",
      "  node_per_graph: 32\n",
      "  pretrain:\n",
      "    atom_bond_only: False\n",
      "    edge_factor: 1.0\n",
      "    graph_factor: 1.0\n",
      "    input_target: True\n",
      "    mask_edge_prob: 0.0\n",
      "    mask_label_prob: 0.5\n",
      "    mask_node_prob: 0.0\n",
      "    node_factor: 1.0\n",
      "    original_task: True\n",
      "    recon: all\n",
      "  radius: extend\n",
      "  sample_node: False\n",
      "  sampler: full_batch\n",
      "  skip_train_eval: False\n",
      "  start_eval_epoch: -1\n",
      "  walk_length: 4\n",
      "val:\n",
      "  node_per_graph: 32\n",
      "  radius: extend\n",
      "  sample_node: False\n",
      "  sampler: full_batch\n",
      "view_emb: False\n",
      "wandb:\n",
      "  project: molhiv\n",
      "  use: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eren/miniconda3/envs/lgd2/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.argv = [\"train_diffusion.py\", \"--cfg\", \"cfg/ogbg-molhiv-diffusion.yaml\", \"--repeat\", '1']\n",
    "args = parse_args()\n",
    "# Load config file\n",
    "set_cfg(cfg)\n",
    "cfg.set_new_allowed(True)\n",
    "load_cfg(cfg, args)\n",
    "cfg.accelerator = 'cpu' #-eren\n",
    "cfg.devices = None\n",
    "loaders = create_loader()\n",
    "train_loader, val_loader, test_loader = loaders\n",
    "print(cfg)\n",
    "\n",
    "# sys.argv = [\"pretrain_encoder.py\", \"--cfg\", \"cfg/ogbg-molhiv-encoder_pretrained.yaml\", \"--repeat\", '1']\n",
    "# args = parse_args()\n",
    "# set_cfg(cfg)\n",
    "# cfg.set_new_allowed(True)\n",
    "# load_cfg(cfg, args)\n",
    "# # print(cfg)\n",
    "# #custom_set_out_dir(cfg, args.cfg_file, cfg.name_tag)\n",
    "# dump_cfg(cfg)\n",
    "# run_id, seed, split_index = 0, 0, 0\n",
    "# # Set configurations for each run\n",
    "# #custom_set_run_dir(cfg, run_id)\n",
    "# set_printing()\n",
    "# cfg.dataset.split_index = split_index\n",
    "# cfg.seed = seed\n",
    "# cfg.run_id = run_id\n",
    "# seed_everything(cfg.seed)\n",
    "# #auto_select_device()\n",
    "\n",
    "# cfg.accelerator = 'cpu' #-eren\n",
    "# cfg.devices = None\n",
    "\n",
    "# TODO: debug loader and dataset\n",
    "# dataset = load_dataset_master(cfg.dataset.format, cfg.dataset.name, cfg.dataset.dataset_dir) # HERE - uncommented this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ccf566-3c4d-49da-9b87-09ef12be356b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in x0-prediction mode\n",
      "DiffusionWrapper has 0.60 M params.\n",
      "Using first stage also as cond stage.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eren/miniconda3/envs/lgd2/lib/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/home/eren/LatentGraphDiffusion/lgd/ddpm/LGD.py:555: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(config, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "model = eval(cfg.model.get('type', 'LatentDiffusion'))\\\n",
    "            (timesteps=cfg.diffusion.get('timesteps', 1000), conditioning_key=cfg.diffusion.conditioning_key,\n",
    "             hid_dim=cfg.diffusion.hid_dim, parameterization=cfg.diffusion.get(\"parameterization\", \"x0\"),\n",
    "             cond_stage_key=cfg.diffusion.cond_stage_key, first_stage_config=cfg.diffusion.first_stage_config,\n",
    "             cond_stage_config=cfg.diffusion.cond_stage_config, edge_factor=cfg.diffusion.get(\"edge_factor\", 1.0),\n",
    "             graph_factor=cfg.diffusion.get(\"graph_factor\", 1.0),\n",
    "             train_mode=cfg.diffusion.get(\"train_mode\", 'sample')).to(torch.device(cfg.accelerator))\n",
    "encoder_model = model.first_stage_model\n",
    "print(encoder_model.training)  # Should print False if in eval mode\n",
    "\n",
    "# def instantiate_first_stage(config):\n",
    "#     first_stage_model = eval(cfg.encoder.get(\"model_type\", \"GraphTransformerEncoder\"))(cfg=cfg.encoder)\n",
    "#     sd = torch.load(config, map_location=\"cpu\")\n",
    "#     if \"state_dict\" in list(sd.keys()):\n",
    "#         sd = sd[\"state_dict\"]\n",
    "#     if \"model_state\" in list(sd.keys()):\n",
    "#         sd = sd[\"model_state\"]\n",
    "\n",
    "#     state_dict = {}\n",
    "#     for k, v in sd.items():\n",
    "#         # new_k = k.replace('model.', '') if 'model' in k else k\n",
    "#         new_k = k[6:] if k.startswith('model.') else k\n",
    "#         state_dict[new_k] = v\n",
    "#     missing, unexpected = first_stage_model.load_state_dict(state_dict, strict=False)\n",
    "#     logging.info(f\"Restored from {config} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
    "#     if len(missing) > 0:\n",
    "#         logging.info(f\"Missing Keys: {missing}\")\n",
    "#     if len(unexpected) > 0:\n",
    "#         logging.info(f\"Unexpected Keys: {unexpected}\")\n",
    "#         first_stage_model = model.eval()\n",
    "#         first_stage_model.train = disabled_train\n",
    "#         for param in first_stage_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#     return first_stage_model\n",
    "# encoder_model = instantiate_first_stage(cfg.diffusion.first_stage_config)\n",
    "# encoder_model.eval()\n",
    "# encoder_model.train = disabled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b07d1ad-2585-4920-8861-0cdeb7786cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n",
      "tensor(3968.)\n",
      "64.0\n",
      "tensor(62.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i, batch in enumerate(train_loader):\n",
    "    \n",
    "    graph_label = batch.y.clone().detach().cpu()\n",
    "    graph_label.squeeze(-1)\n",
    "    #print('graph_label', graph_label)\n",
    "    batch_z = model.encode_first_stage(batch)\n",
    "    node_decode, edge_decode, graph_decode = model.decode_first_stage(batch_z)\n",
    "    #print(graph_decode)\n",
    "    probabilities = torch.sigmoid(graph_decode).cpu()\n",
    "    hard_pred = (probabilities >= 0.5).int()\n",
    "    assert(graph_decode.shape[0] == graph_label.shape[0])\n",
    "    print(graph_label.shape)\n",
    "    correct += torch.sum(hard_pred == graph_label)\n",
    "    \n",
    "    \n",
    "    total += graph_decode.shape[0]\n",
    "    print(correct)\n",
    "    print(total)\n",
    "    break\n",
    "\n",
    "print(correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ce9746-0ebb-4c09-9f42-8266e94dd73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9623111759520987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "true = []\n",
    "pred = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    \n",
    "    graph_label = batch.y.clone().detach()\n",
    "    batch_z = model.encode_first_stage(batch)\n",
    "    node_decode, edge_decode, graph_decode = model.decode_first_stage(batch_z)\n",
    "    probabilities = torch.sigmoid(graph_decode)\n",
    "    hard_pred = (probabilities >= 0.5).int()\n",
    "\n",
    "    true.append(graph_label)\n",
    "    pred.append(hard_pred)\n",
    "\n",
    "true = torch.cat(true, dim=0)\n",
    "pred = torch.cat(pred, dim=0)\n",
    "\n",
    "print(accuracy_score(true, pred))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c29f6a55-fb5d-4388-ab80-0584a978a40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eren/miniconda3/envs/lgd2/lib/python3.9/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6819175635930587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    graph_label = batch.y.clone().detach()\n",
    "    \n",
    "    batch_z = model.encode_first_stage(batch)\n",
    "    node_decode, edge_decode, graph_decode = model.decode_first_stage(batch_z)\n",
    "\n",
    "    probabilities = torch.sigmoid(graph_decode)  # Use probabilities for AUC\n",
    "\n",
    "    all_labels.append(graph_label.cpu().numpy())  # Store labels\n",
    "    all_probs.append(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "all_probs = np.concatenate(all_probs, axis=0)\n",
    "\n",
    "# Compute AUC score\n",
    "auc_score = roc_auc_score(all_labels, all_probs)\n",
    "print(\"AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3951bac-d0b6-4749-acca-47dab35ad00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_acc(model, loader):\n",
    "    graph_label = batch.y.clone().detach()\n",
    "    #print('graph_label', graph_label)\n",
    "    batch_z = encoder_model(batch, label = graph_label)\n",
    "    #print('batch', batch)\n",
    "    print()\n",
    "    #print('batch_z', batch_z)\n",
    "    #print()\n",
    "    node_decode, edge_decode, graph_decode = encoder_model.decode(batch_z)\n",
    "    probabilities = F.softmax(graph_decode, dim=-1)  # Softmax over the class dimension\n",
    "\n",
    "    print(probabilities)\n",
    "\n",
    "    torch.sum(graph_decode == graph_label)\n",
    "def fidelity(model, loader):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
