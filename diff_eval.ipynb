{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90ae34a-8ff5-4e81-9055-9933bdc60699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aga\n",
      "lgd2\n"
     ]
    }
   ],
   "source": [
    "print('aga')\n",
    "!echo $CONDA_DEFAULT_ENV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e36b1ad-297a-4b5d-976a-cedc9366dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lgd  # noqa, register custom modules\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from torch_geometric.graphgym.cmd_args import parse_args\n",
    "from torch_geometric.graphgym.config import (cfg, dump_cfg,\n",
    "                                             set_cfg, load_cfg,\n",
    "                                             makedirs_rm_exist)\n",
    "from torch_geometric.graphgym.loader import create_loader\n",
    "from torch_geometric.graphgym.logger import set_printing\n",
    "from torch_geometric.graphgym.optim import create_optimizer, \\\n",
    "    create_scheduler, OptimizerConfig\n",
    "from torch_geometric.graphgym.model_builder import create_model\n",
    "from torch_geometric.graphgym.train import GraphGymDataModule, train\n",
    "from torch_geometric.graphgym.utils.comp_budget import params_count\n",
    "from torch_geometric.graphgym.utils.device import auto_select_device\n",
    "from torch_geometric.graphgym.register import train_dict, network_dict, register_network, act_dict\n",
    "from torch_geometric import seed_everything\n",
    "from lgd.asset.logger import create_logger\n",
    "from lgd.loader.master_loader import load_dataset_master\n",
    "from lgd.optimizer.extra_optimizers import ExtendedSchedulerConfig\n",
    "from lgd.agg_runs import agg_runs\n",
    "from lgd.finetuning import load_pretrained_model_cfg, \\\n",
    "    init_model_from_pretrained\n",
    "from lgd.ddpm.LGD import DDPM, LatentDiffusion\n",
    "from lgd.ddpm.LGD_Inductive import LatentDiffusionInductive\n",
    "from lgd.train.pretrain_encoder import *\n",
    "from lgd.encoder.atom_bond_encoder import *\n",
    "from lgd.model.GraphTransformerEncoder import GraphTransformerEncoder # HERE \n",
    "from lgd.train.train_diffusion import *\n",
    "from utils import print_gpu_usage\n",
    "import sys\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46aebe06-1ac4-4f9b-925d-3aa05ced47f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format OGB\n",
      "here ogbg-molhiv\n",
      "if ten once ./datasets/ogbg_molhiv/processed.pt\n",
      "if ici\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eren/LatentGraphDiffusion/lgd/loader/master_loader.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(processed_path, map_location=\"cpu\")\n",
      "/home/eren/miniconda3/envs/lgd2/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.argv = [\"train_diffusion.py\", \"--cfg\", \"cfg/ogbg-molhiv-diffusion.yaml\", \"--repeat\", '1']\n",
    "args = parse_args()\n",
    "# Load config file\n",
    "set_cfg(cfg)\n",
    "cfg.set_new_allowed(True)\n",
    "load_cfg(cfg, args)\n",
    "cfg.accelerator = 'cpu' #-eren\n",
    "cfg.devices = None\n",
    "loaders = create_loader()\n",
    "train_loader, val_loader, test_loader = loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ccf566-3c4d-49da-9b87-09ef12be356b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in x0-prediction mode\n",
      "DiffusionWrapper has 0.60 M params.\n",
      "Using first stage also as cond stage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eren/miniconda3/envs/lgd2/lib/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/home/eren/LatentGraphDiffusion/lgd/ddpm/LGD.py:555: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(config, map_location=\"cpu\")\n",
      "/home/eren/LatentGraphDiffusion/lgd/finetuning.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_file, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "model = eval(cfg.model.get('type', 'LatentDiffusion'))\\\n",
    "            (timesteps=cfg.diffusion.get('timesteps', 1000), conditioning_key=cfg.diffusion.conditioning_key,\n",
    "             hid_dim=cfg.diffusion.hid_dim, parameterization=cfg.diffusion.get(\"parameterization\", \"x0\"),\n",
    "             cond_stage_key=cfg.diffusion.cond_stage_key, first_stage_config=cfg.diffusion.first_stage_config,\n",
    "             cond_stage_config=cfg.diffusion.cond_stage_config, edge_factor=cfg.diffusion.get(\"edge_factor\", 1.0),\n",
    "             graph_factor=cfg.diffusion.get(\"graph_factor\", 1.0),\n",
    "             train_mode=cfg.diffusion.get(\"train_mode\", 'sample')).to(torch.device(cfg.accelerator))\n",
    "\n",
    "pretrained_dir = 'results/ogbg-molhiv-diffusion-ogbg-molhiv-orig-2025-03-02_15-14-20/0/ckpt/49.ckpt'\n",
    "model = init_model_from_pretrained(\n",
    "                model, pretrained_dir, cfg.pretrained.freeze_main,\n",
    "                cfg.pretrained.reset_prediction_head\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b07d1ad-2585-4920-8861-0cdeb7786cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(model, loader):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, batch in enumerate(loader):\n",
    "        \n",
    "        graph_label = batch.y.clone().detach().cpu()\n",
    "        graph_label = graph_label.squeeze(-1)\n",
    "        batch_z = model.encode_first_stage(batch)\n",
    "        node_decode, edge_decode, graph_decode = model.decode_first_stage(batch_z)\n",
    "        graph_decode = graph_decode.squeeze(-1)\n",
    "        probabilities = torch.sigmoid(graph_decode).cpu()\n",
    "        hard_pred = (probabilities >= 0.5).int()\n",
    "        assert(graph_decode.shape[0] == graph_label.shape[0])\n",
    "        correct += torch.sum(hard_pred == graph_label)\n",
    "        \n",
    "        total += graph_decode.shape[0]\n",
    "    \n",
    "    print(correct/total)\n",
    "    return correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3951bac-d0b6-4749-acca-47dab35ad00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling t: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [32:03<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "time_start = time.time()\n",
    "generated_graphs = []\n",
    "iter = 0\n",
    "num_test_graphs = 0\n",
    "for batch in test_loader:\n",
    "    batch.split = 'test'\n",
    "    batch.to(torch.device(cfg.accelerator))\n",
    "    \n",
    "    node_label, edge_label, graph_label = batch.x.clone().detach().flatten(), batch.edge_attr.clone().detach().flatten(), batch.y\n",
    "    # the embed of labels and prefix are done in fine-tuning of the encoder, not pretraining\n",
    "    batch.x_masked = batch.x.clone().detach()\n",
    "    batch.edge_attr_masked = batch.edge_attr.clone().detach()\n",
    "    ddim_steps = cfg.diffusion.get('ddim_steps', None)\n",
    "    ddim_eta = cfg.diffusion.get('ddim_eta', 0.0)\n",
    "    use_ddpm_steps = cfg.diffusion.get('use_ddpm_steps', False)\n",
    "\n",
    "    _, node_decode, edge_decode, graph_decode, generated = model.inference(batch, ddim_steps=ddim_steps, ddim_eta=ddim_eta, use_ddpm_steps=use_ddpm_steps, visualize=False, return_all = True)\n",
    "    for each in generated:\n",
    "        generated_graphs.append(each)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2731c6e-c49f-454f-8666-f95de8541b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('generated_tuple.pkl', 'wb') as f:\n",
    "    pickle.dump((node_decode, edge_decode, graph_decode, generated), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38e67ab3-bd67-45ad-98b2-c2618eb2e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3253])\n",
      "torch.Size([102789])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 20])\n"
     ]
    }
   ],
   "source": [
    "with open('generated_tuple.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)  \n",
    "    node_decode, edge_decode, graph_decode, generated = loaded_data\n",
    "\n",
    "\n",
    "\n",
    "for batch in test_loader:\n",
    "    #BURA ONEMLI\n",
    "    node_label = batch.x.clone().detach()\n",
    "    edge_label = batch.edge_attr.clone().detach()\n",
    "    if not cfg.train.pretrain.atom_bond_only:\n",
    "        node_label = node_label[:, 0].flatten()\n",
    "        edge_label = edge_label[:, 0].flatten()\n",
    "\n",
    "    break\n",
    "print(node_label.shape)\n",
    "print(edge_label.shape)\n",
    "gen_node, gen_edge, gen_graph = generated[0]\n",
    "print(gen_node.shape)\n",
    "print(gen_edge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15989222-3e4a-413d-8a7b-81e665b79ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_pred_probs(batch, model):\n",
    "    graph_label = batch.y.clone().detach().cpu()\n",
    "    graph_label = graph_label.squeeze(-1)\n",
    "    batch_z = model.encode_first_stage(batch)\n",
    "    node_decode, edge_decode, graph_decode = model.decode_first_stage(batch_z)\n",
    "    graph_decode = graph_decode.squeeze(-1)\n",
    "    probabilities = torch.sigmoid(graph_decode).cpu()\n",
    "\n",
    "    return probabilities, batch_z, node_decode, edge_decode, graph_decode\n",
    "\n",
    "@torch.no_grad()\n",
    "def fidelity(model, decoded, loader):\n",
    "    # all_orig_probs = []\n",
    "    # all_cf_probs = []\n",
    "    fid_drops = []\n",
    "    for i, batch in loader:\n",
    "        graph_label = batch.y.clone().detach().cpu()\n",
    "        graph_label = graph_label.squeeze(-1).int()\n",
    "        \n",
    "        orig_probs, _, _, _, _ = get_pred_probs(batch, model) #bs\n",
    "        orig_probs = torch.stack(1-orig_probs, orig_probs, dim=1) #bs x 2\n",
    "        #all_orig_probs.append(orig_probs)\n",
    "        \n",
    "        graph_decode, node_decode, edge_decode = decoded[i]\n",
    "        decoded_batch = batch\n",
    "        print('decoded_batch', decoded_batch)\n",
    "        decoded_batch.x = node_decode\n",
    "        decoded_batch.edge_decode = edge_decode\n",
    "        \n",
    "\n",
    "        #either calculate with graph_decode which is kindddda cheating bc we dont construct graph and\n",
    "        #embed back to alte\n",
    "        cf_probs, _, _, _, _ = get_pred_probs(decoded_batch, model)\n",
    "        orig_probs = torch.stack(1-orig_probs, orig_probs, dim=1) #bs x 2\n",
    "        \n",
    "        fid_drop = (1 - cf_probs.gather(1, labels).view(-1)).detach().cpu().numpy()\n",
    "        fid_drop = np.mean(fid_drop)\n",
    "        \n",
    "        #all_labels.append(graph_label)\n",
    "\n",
    "    all_probs = torch.cat(all_probs, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_cf(model, loader):\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ce8e7-6c16-43fb-8180-e14b8eb7a5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
